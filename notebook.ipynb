{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import  data_reader, data_loader\n",
    "from modules.sr import result_saver\n",
    "from modules.models import lag_llama\n",
    "from modules.experiment.tscv import get_tscv_results, get_summary, extract_metrics\n",
    "from modules.visualization import graphs\n",
    "from modules.fine_tuning import lag_llama_ft\n",
    "from modules.models import prpht\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earliest monthly data for S&P500 is 1985\n",
    "FTSE 100 also 1985\n",
    "NASDAQ also 1985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from modules.data import data_loader\n",
    "\n",
    "a = data_loader.get_data(type = \"index\", ticker=\"S&P 500\", frequency = \"weekly\", start = \"2017-01-01\", end = \"2022-01-01\", rtrn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>-0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-15</td>\n",
       "      <td>-0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>0.010294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>0.008131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2020-10-04</td>\n",
       "      <td>0.038442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>0.001918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2020-10-18</td>\n",
       "      <td>-0.005287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2020-10-25</td>\n",
       "      <td>-0.056395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>0.073236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds         y\n",
       "1   2017-01-08 -0.001028\n",
       "2   2017-01-15 -0.001464\n",
       "3   2017-01-22  0.010294\n",
       "4   2017-01-29  0.001190\n",
       "5   2017-02-05  0.008131\n",
       "..         ...       ...\n",
       "196 2020-10-04  0.038442\n",
       "197 2020-10-11  0.001918\n",
       "198 2020-10-18 -0.005287\n",
       "199 2020-10-25 -0.056395\n",
       "200 2020-11-01  0.073236\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = data_loader.get_data(type = \"cc\", frequency = \"weekly\", ticker = \"Social\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_loader.get_data(type = \"fx\", ticker = \"USD/GBP\", frequency = \"daily\", rtrn = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[\"Date\", \"Aggregate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import data_loader\n",
    "\n",
    "a = data_loader.get_data(type = \"index\",ticker=\"NASDAQ\", frequency = \"daily\", start = \"2022-01-01\", end = \"2024-01-01\", rtrn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data_loader.get_data(type = \"index\",ticker=\"S&P 500\", frequency = \"daily\", start = \"2022-01-01\", end = \"2024-01-01\", rtrn = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models import arima_hybrid\n",
    "\n",
    "pr = arima_hybrid.get_arima_timegpt_predictions(data=a, prediction_horizon=1, frequency=\"daily\", error_train_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array([1, 2, 3]) - np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import data_loader\n",
    "\n",
    "#a = data_loader.get_data(type = \"commodity\", ticker = \"WTI\", frequency = \"weekly\", start = \"2019-01-01\", end = \"2024-01-01\", rtrn = True)\n",
    "#a = data_loader.get_data(type = \"stock\", ticker = \"IBM\", frequency = \"daily\", start = \"01-01-2023\", end = \"01-01-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data_loader.get_data(type = \"fx\", ticker = \"USD/GBP\", frequency = \"weekly\", start = \"2015-01-01\", end = \"2020-01-01\", rtrn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>0.011656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-23</td>\n",
       "      <td>0.011067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>-0.004049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>-0.012496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>-0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>-0.014963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>-0.015427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>0.025390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>-0.005709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds         y\n",
       "1   2015-01-09  0.011656\n",
       "2   2015-01-16  0.000000\n",
       "3   2015-01-23  0.011067\n",
       "4   2015-01-30 -0.004049\n",
       "5   2015-02-06 -0.012496\n",
       "..         ...       ...\n",
       "256 2019-11-29 -0.007394\n",
       "257 2019-12-06 -0.014963\n",
       "258 2019-12-13 -0.015427\n",
       "259 2019-12-20  0.025390\n",
       "260 2019-12-27 -0.005709\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "c = pd.read_csv(\"results/prediction/T=WTI_FR=weekly_TOD=commodity_FO=5_CLTS=32_SD=2019-01-01_ED=2024-01-01_FTL=200_FTF=5_FTG=0_TSCVR=6_BS=20_ME=4.csv\", index_col = 0)\n",
    "d = pd.read_csv(\"results/evaluation/T=WTI_FR=weekly_TOD=commodity_FO=5_CLTS=32_SD=2019-01-01_ED=2024-01-01_FTL=200_FTF=5_FTG=0_TSCVR=6_BS=20_ME=4.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arima</th>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoregressor</th>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prophet</th>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_llama</th>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_lag_llama</th>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeGPT</th>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_timeGPT</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mes\n",
       "arima          0.866667\n",
       "autoregressor  0.466667\n",
       "prophet        0.366667\n",
       "lag_llama      0.466667\n",
       "ft_lag_llama   0.466667\n",
       "timeGPT        0.433333\n",
       "ft_timeGPT     0.500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.data import metrics\n",
    "\n",
    "metrics.mean_equal_sign(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"abc\" in \"ajskfhabcahsh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the folders\n",
    "folder1 = \"results/evaluation\"\n",
    "folder2 = \"results/prediction\"\n",
    "\n",
    "# List all CSV files in the second folder\n",
    "files_in_folder2 = [f for f in os.listdir(folder2) if f.endswith('.csv')]\n",
    "\n",
    "for file in files_in_folder2:\n",
    "    \n",
    "    # Full paths to the files\n",
    "    file_path1 = os.path.join(folder1, file)\n",
    "    file_path2 = os.path.join(folder2, file)\n",
    "    \n",
    "    # Read the CSV files into dataframes\n",
    "    df1 = pd.read_csv(file_path1, index_col=0)\n",
    "    df2 = pd.read_csv(file_path2, index_col=0)\n",
    "    try:\n",
    "        mes = metrics.mean_equal_sign(df2)\n",
    "    except ZeroDivisionError:\n",
    "        print(file)\n",
    "        break\n",
    "    \n",
    "    df_merged = df1.join(mes)\n",
    "    \n",
    "    \n",
    "    # Save the updated dataframe back to the first folder\n",
    "    df_merged.to_csv(\"results/evaluation_with_mes/\"+file)\n",
    "\n",
    "print(\"All files have been updated successfully.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the folders\n",
    "folder1 = \"results/evaluation\"\n",
    "folder2 = \"results/prediction\"\n",
    "\n",
    "# List all CSV files in the second folder\n",
    "files_in_folder2 = [f for f in os.listdir(folder2) if f.endswith('.csv')]\n",
    "\n",
    "for file in files_in_folder2:\n",
    "    if \"TOD=cc\" in file:\n",
    "        continue\n",
    "    # Full paths to the files\n",
    "    file_path1 = os.path.join(folder1, file)\n",
    "    file_path2 = os.path.join(folder2, file)\n",
    "    \n",
    "    # Read the CSV files into dataframes\n",
    "    df1 = pd.read_csv(file_path1, index_col=0)\n",
    "    df2 = pd.read_csv(file_path2, index_col=0)\n",
    "    try:\n",
    "        mes = metrics.mean_equal_sign(df2)\n",
    "    except ZeroDivisionError:\n",
    "        print(file)\n",
    "        break\n",
    "    \n",
    "    df_merged = df1.join(mes)\n",
    "    \n",
    "    \n",
    "    # Save the updated dataframe back to the first folder\n",
    "    df_merged.to_csv(\"results/evaluation_with_mes/\"+file)\n",
    "\n",
    "print(\"All files have been updated successfully.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>0.080426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>0.022450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>0.018490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>0.014183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>-0.001865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>-0.019865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>-0.063320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>-0.008485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>0.044501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.003960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds         y\n",
       "1   2019-01-11  0.080426\n",
       "2   2019-01-18  0.022450\n",
       "3   2019-01-25  0.018490\n",
       "4   2019-02-01  0.014183\n",
       "5   2019-02-08 -0.001865\n",
       "..         ...       ...\n",
       "256 2023-12-01 -0.019865\n",
       "257 2023-12-08 -0.063320\n",
       "258 2023-12-15 -0.008485\n",
       "259 2023-12-22  0.044501\n",
       "260 2023-12-29  0.003960\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00, -6.18616706e-02, -4.19867743e-02,  6.98279699e-02,\n",
       "        4.24912810e-02, -2.31593135e-03, -5.60290229e-02, -8.77903779e-02,\n",
       "       -3.51976251e-02, -3.62018366e-02,  9.71060450e-03,  8.28603460e-03,\n",
       "       -6.91259556e-03, -3.59815971e-02, -2.49966042e-02, -2.62490790e-02,\n",
       "       -1.90792718e-02, -1.39096499e-03, -1.57950872e-02, -3.02366138e-02,\n",
       "        1.32687974e-02, -1.29360540e-02, -6.00174951e-03, -2.43036566e-03,\n",
       "       -8.95217317e-03, -6.03774081e-04, -3.41447796e-02,  5.85180692e-03,\n",
       "        1.83835276e-02,  1.06312754e-02, -5.13947677e-03, -2.56214051e-02,\n",
       "        6.16410492e-03,  7.30080507e-03,  5.26872738e-03, -1.78165810e-03,\n",
       "        7.72739448e-03,  1.51210900e-02, -2.63554160e-02, -6.01056327e-03,\n",
       "        9.56645248e-03,  1.49142905e-03,  1.39029956e-02, -6.26376911e-03,\n",
       "        3.07570637e-03,  1.69115687e-02, -8.76883165e-03, -3.60830801e-02,\n",
       "       -3.12340150e-02, -2.53231741e-02,  6.50392387e-03, -2.05947294e-03,\n",
       "       -9.49195682e-03, -2.17939293e-04, -1.87504963e-02, -1.52534333e-02,\n",
       "        1.29824182e-02,  3.53129043e-03,  6.71321261e-03,  6.26362914e-03,\n",
       "       -5.78309173e-03, -2.61939797e-02, -4.27465044e-03, -9.15917882e-03,\n",
       "       -2.16431617e-02,  8.62409400e-03, -2.10762400e-02, -1.10265120e-02,\n",
       "        1.10736387e-03,  1.80964022e-02,  2.93213552e-03, -2.93805454e-03,\n",
       "        6.05373592e-03, -8.08760533e-03,  1.55575608e-02,  1.34995057e-02,\n",
       "        1.26574092e-02,  4.65850927e-04, -1.23821624e-02, -1.70614723e-02,\n",
       "       -1.18732639e-02, -1.67281566e-02, -7.95345489e-03, -4.97228411e-02,\n",
       "        2.78004217e-02, -7.30516322e-03,  5.29757937e-03,  1.77780699e-02,\n",
       "        3.49717063e-03,  1.06818908e-02,  6.25511703e-03,  1.05929046e-03,\n",
       "       -4.60017943e-04,  3.35260283e-03,  8.83123548e-03, -6.09789900e-03,\n",
       "        6.22703753e-02,  7.28900051e-03, -5.08409335e-02,  3.82923529e-02,\n",
       "       -5.02238771e-02, -1.51836396e-02,  4.21457541e-03,  1.96183531e-02,\n",
       "       -5.02471929e-03,  1.34625796e-02, -8.68694215e-03,  1.87695794e-02,\n",
       "       -3.11596781e-03,  3.65045846e-03,  5.35194382e-03, -2.53038541e-02,\n",
       "       -3.88692975e-02,  9.87371133e-04, -3.92350646e-02, -9.29293887e-03,\n",
       "        4.58294396e-03, -2.12121655e-02, -2.71906702e-02, -1.35857633e-02,\n",
       "       -2.19897571e-02,  6.71508833e-03, -2.58504245e-02, -3.20083713e-02,\n",
       "       -3.40097640e-03, -2.75273415e-02, -2.25269649e-02,  3.63502071e-02,\n",
       "       -9.18998667e-03, -3.59617454e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "pacf(a[\"y\"], nlags=len(a)//2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\topco\\Dokumenti\\MSc Banking and Digital Finance UCL\\Modules\\Dissertation\\MSc_dissertation\\modules\\data\\time_series_analysis.py:54: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  results['residual_volatility'] = volatility / noise_level\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stationary': True,\n",
       " 'adf_statistic': -17.05377860623053,\n",
       " 'linear_trend': False,\n",
       " 'linear_tau': -0.04015444015444015,\n",
       " 'quadratic_trend': False,\n",
       " 'quadratic_tau': 0.029937629937629932,\n",
       " 'cyclical_patterns': False,\n",
       " 'residual_mean': 0.0,\n",
       " 'residual_volatility': nan}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.data import time_series_analysis\n",
    "\n",
    "time_series_analysis.analyze_time_series(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\topco\\Dokumenti\\MSc Banking and Digital Finance UCL\\Modules\\Dissertation\\MSc_dissertation\\modules\\data\\data_loader.py:129: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['y'] = df['y'].fillna(method='bfill') # imputing the missing values\n"
     ]
    }
   ],
   "source": [
    "x = data_loader.get_exogenous_data(start_date = \"1987-01-01\", end_date = \"2024-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\topco\\AppData\\Roaming\\Python\\Python311\\site-packages\\nixtlats\\__init__.py:5: FutureWarning: This package is deprecated, please install nixtla instead.\n",
      "  warnings.warn(\"This package is deprecated, please install nixtla instead.\", category=FutureWarning)\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Restricting input...\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Restricting input...\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Restricting input...\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Using the following exogenous variables: x1, x2, x3\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
     ]
    }
   ],
   "source": [
    "from modules.models import timegpt\n",
    "\n",
    "pr = timegpt.get_timegpt_forecast(data=a, prediction_length=1, frequency=\"monthly\", x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3872.1965905683364]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# this adds partial evaluation rows to evaluation folder\n",
    "\n",
    "\"\"\"import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the folders\n",
    "folder1 = \"results/evaluation\"\n",
    "folder2 = \"results/partial/evaluation\"\n",
    "\n",
    "# List all CSV files in the second folder\n",
    "files_in_folder2 = [f for f in os.listdir(folder2) if f.endswith('.csv')]\n",
    "\n",
    "for file in files_in_folder2:\n",
    "    # Full paths to the files\n",
    "    file_path1 = os.path.join(folder1, file)\n",
    "    file_path2 = os.path.join(folder2, file)\n",
    "    \n",
    "    # Read the CSV files into dataframes\n",
    "    df1 = pd.read_csv(file_path1, index_col=0)\n",
    "    df2 = pd.read_csv(file_path2, index_col=0)\n",
    "    \n",
    "    # Update df1 with rows from df2, replacing rows with the same index\n",
    "    df1.update(df2)\n",
    "    \n",
    "    # Append the remaining rows from df2 that are not in df1\n",
    "    df1 = df1.combine_first(df2)\n",
    "    \n",
    "    # Save the updated dataframe back to the first folder\n",
    "    df1.to_csv(file_path1)\n",
    "\n",
    "print(\"All files have been updated successfully.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "folder1 = \"results/prediction\"\n",
    "folder2 = \"results/partial/prediction\"\n",
    "\n",
    "# Get list of files in both folders\n",
    "files_folder1 = set(os.listdir(folder1))\n",
    "files_folder2 = set(os.listdir(folder2))\n",
    "\n",
    "# Loop through the files in the second folder\n",
    "for file_name in files_folder2:\n",
    "    if file_name in files_folder1:\n",
    "        # Load the dataframes\n",
    "        df1 = pd.read_csv(os.path.join(folder1, file_name))\n",
    "        df2 = pd.read_csv(os.path.join(folder2, file_name))\n",
    "        \n",
    "        merged_df = pd.merge(df1, df2, on=\"actual\", how=\"inner\", suffixes=('_df1', ''))\n",
    "\n",
    "        # Drop the columns from df1 that have the same name as those in df2\n",
    "        columns_to_drop = [col for col in merged_df.columns if col.endswith('_df1')]\n",
    "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Save the merged dataframe back to the first folder\n",
    "        merged_df.to_csv(os.path.join(folder1, file_name), index=False)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the directory containing the CSV files\n",
    "directory = \"results/prediction\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a CSV\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if \"Unnamed: 0\" column exists\n",
    "        if \"Unnamed: 0\" in df.columns:\n",
    "            # Set \"Unnamed: 0\" as the index and remove its name\n",
    "            df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "            df.index.name = None\n",
    "            \n",
    "            # Save the modified DataFrame back to the same file\n",
    "            df.to_csv(file_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_loader.get_exogenous_data(start_date=\"1920-01-01\", end_date=\"2024-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.read_csv(os.path.join(\"data\", \"credit_card\", \"uk_credit_card_data_monthly.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files are exclusively in 'evaluation'.\n",
      "No files are exclusively in 'prediction'.\n"
     ]
    }
   ],
   "source": [
    "# This is for seeing if the evaluation and prediction contain the same files\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the paths to the folders\n",
    "evaluation_folder = \"results/evaluation\"\n",
    "prediction_folder = \"results/prediction\"\n",
    "\n",
    "# List the files in each folder\n",
    "evaluation_files = set(os.listdir(evaluation_folder))\n",
    "prediction_files = set(os.listdir(prediction_folder))\n",
    "\n",
    "# Find files that are in evaluation but not in prediction\n",
    "evaluation_not_in_prediction = evaluation_files - prediction_files\n",
    "\n",
    "# Find files that are in prediction but not in evaluation\n",
    "prediction_not_in_evaluation = prediction_files - evaluation_files\n",
    "\n",
    "# Print the results\n",
    "if evaluation_not_in_prediction:\n",
    "    print(\"Files in 'evaluation' but not in 'prediction':\")\n",
    "    for file in evaluation_not_in_prediction:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No files are exclusively in 'evaluation'.\")\n",
    "\n",
    "if prediction_not_in_evaluation:\n",
    "    print(\"\\nFiles in 'prediction' but not in 'evaluation':\")\n",
    "    for file in prediction_not_in_evaluation:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No files are exclusively in 'prediction'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# this is for renaming files\n",
    "\n",
    "#import os\n",
    "'''def rename_files_in_folder(folder_path, old_string, new_string):\n",
    "    try:\n",
    "        # Get the list of all files in the specified folder\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        for filename in files:\n",
    "            # Check if the old string is in the filename\n",
    "            if old_string in filename:\n",
    "                # Create the new filename by replacing the old string with the new string\n",
    "                new_filename = filename.replace(old_string, new_string)\n",
    "                \n",
    "                # Construct the full file paths\n",
    "                old_file_path = os.path.join(folder_path, filename)\n",
    "                new_file_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(old_file_path, new_file_path)\n",
    "                \n",
    "                print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "            else:\n",
    "                print(f\"No match found in: {filename}\")\n",
    "                \n",
    "        print(\"Renaming process completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "folder_path = os.path.abspath(os.path.join(os.getcwd(), \"results\", \"partial\", \"prediction\")) #\"/results/evaluation\"\n",
    "folder_path_2 = os.path.abspath(os.path.join(os.getcwd(), \"results\", \"partial\", \"evaluation\"))\n",
    "\n",
    "s1 = \"__\"\n",
    "s2 = \"_\"\n",
    "\n",
    "rename_files_in_folder(folder_path, s1, s2)\n",
    "rename_files_in_folder(folder_path_2, s1, s2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming process completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# this is for adding something at the end\n",
    "\n",
    "import os\n",
    "\n",
    "# this is for renaming files\n",
    "\n",
    "def rename_files_in_folder(folder_path):\n",
    "    try:\n",
    "        # Get the list of all files in the specified folder\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        for filename in files:\n",
    "            # Check if the old string is in the filename\n",
    "            \n",
    "                old_file_path = os.path.join(folder_path, filename)\n",
    "                new_file_path = old_file_path[:-4] + \"_LR=0.0005.csv\"\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(old_file_path, new_file_path)\n",
    "                \n",
    "                \n",
    "                \n",
    "        print(\"Renaming process completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "#folder_path = os.path.abspath(os.path.join(os.getcwd(), \"results\", \"prediction\")) #\"/results/evaluation\"\n",
    "folder_path_2 = os.path.abspath(os.path.join(os.getcwd(), \"results\", \"evaluation_with_mes\"))\n",
    "\n",
    "\n",
    "\n",
    "#rename_files_in_folder(folder_path)\n",
    "rename_files_in_folder(folder_path_2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-4 == 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this combines ft_timeGPT evaluation with previous results\n",
    "\n",
    "'''import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to the folders\n",
    "folder1 = \"results/prediction\"\n",
    "folder2 = \"results/partial/prediction\"\n",
    "\n",
    "# Get a list of all .csv files in both folders\n",
    "files1 = [f for f in os.listdir(folder1) if f.endswith('.csv')]\n",
    "files2 = [f for f in os.listdir(folder2) if f.endswith('.csv')]\n",
    "\n",
    "# Ensure that only matching files are processed\n",
    "matching_files = set(files1) & set(files2)\n",
    "\n",
    "# Loop through each matching file\n",
    "for filename in matching_files:\n",
    "    # Construct the full file paths\n",
    "    file1_path = os.path.join(folder1, filename)\n",
    "    file2_path = os.path.join(folder2, filename)\n",
    "    \n",
    "    # Read the dataframes from both files\n",
    "    df1 = pd.read_csv(file1_path, index_col=0)\n",
    "    df2 = pd.read_csv(file2_path, index_col=0)\n",
    "    \n",
    "    # Concatenate the dataframes, preserving the index\n",
    "    combined_df = pd.merge(df1, df2, on=['actual', 'timestamp'])\n",
    "    \n",
    "    # Save the combined dataframe back to the file in the first folder\n",
    "    combined_df.to_csv(file1_path)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "#constraint: FOLDS x PREDICTION_LENGTH + TRAIN_SIZE !!must not!! be bigger than len(data)\n",
    "\n",
    "PREDICTION_LENGTH = 1 # currentlz only works for PREDICTION_LENGTH > 1\n",
    "TICKER = \"S&P 500\"\n",
    "FREQUENCY = \"minutely\" # currently we only have dailz frequency\n",
    "TYPE_OF_DATA = \"stock\" # currently we only have stock prices saved\n",
    "MODELS = [\"arima\", \"llama\", \"autoregressor\", \"fine-tuned Llama\"] # currentlz works onlz for these two\n",
    "FOLDS = 5 # for TSCV # reduced to two for testing purposes\n",
    "CONTEXT_LENGTH = 245 # set to 245 for testing purposes\n",
    "METRICS = ['r2', 'mse', 'mae', 'rmse', 'mda', \"mape\"]\n",
    "\n",
    "# fine-tuning parameters\n",
    "BATCH_SIZE = 10\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "# data parameters\n",
    "START_DATE = \"2024-07-22\"\n",
    "END_DATE = \"2024-07-23\"\n",
    "\n",
    "# want to add\n",
    "#TRAIN_PERIOD = # context lenghts. Should take a look into this\n",
    "TRAIN_SIZE = CONTEXT_LENGTH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = data_reader.read_data(type = TYPE_OF_DATA, frequency = FREQUENCY)\n",
    "# this is just in case there are more CSVs of the same type and frequency, the data should be the first in the list\n",
    "#if len(data) > 1:\n",
    "data = data[0]\n",
    "\n",
    "simple_data = data_reader.read_data(type = TYPE_OF_DATA, frequency = FREQUENCY, match = [\"simple\"])\n",
    "#if len(simple_data) > 1:\n",
    "simple_data = simple_data[0]\n",
    "\n",
    "train_data = data_reader.read_data(type = TYPE_OF_DATA, frequency = FREQUENCY, match = [\"train\"])\n",
    "train_data = train_data[0]\n",
    "\n",
    "test_data = data_reader.read_data(type = TYPE_OF_DATA, frequency = FREQUENCY, match = [\"test\"])\n",
    "test_data = test_data[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CONFIG = {\"ticker\" : TICKER,\n",
    "               \"frequency\" : FREQUENCY,\n",
    "               \"start\" : START_DATE,\n",
    "               \"end\" : END_DATE}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader.get_data(data_type=TYPE_OF_DATA, kwargs=DATA_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LENGTH = len(data)\n",
    "FOLDS = int((DATA_LENGTH - TRAIN_SIZE) / PREDICTION_LENGTH) # this calculates max ammount of folds we can have given a set TRAIN_SIZE and PREDICTION_LENGTH\n",
    "FOLDS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"ticker\" : \"USD/GBP\",\n",
    "          \"frequency\" : \"daily\",\n",
    "          \"start\" : \"2024-07-01\",\n",
    "          \"end\" : \"2024-07-30\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"ALPHAVANTAGE_API\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"ALPHAVANTAGE_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in os.environ.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = data_loader.get_data(data_type=\"exchange_rate\", kwargs=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the training data for lag llama fine tuning\n",
    "ft_train_data = lag_llama.prepare_data(data=ft_data, \n",
    "                                       prediction_length=0, \n",
    "                                       frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the lag llama predictor object \n",
    "predictor = lag_llama_ft.get_predictor(prediction_length=PREDICTION_LENGTH, \n",
    "                                       context_length=CONTEXT_LENGTH, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       max_epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning the predictor object\n",
    "predictor = predictor.train(ft_train_data, \n",
    "                            cache_data = True, \n",
    "                            shuffle_buffer_length = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = get_tscv_results(data = data,\n",
    "                           prediction_horizon=PREDICTION_LENGTH,\n",
    "                           context_length=CONTEXT_LENGTH, \n",
    "                           folds=FOLDS, \n",
    "                           frequency=FREQUENCY,\n",
    "                           predictor=predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"PREDICTION_LENGTH={PREDICTION_LENGTH}__TICKER={TICKER}__FREQUENCY={FREQUENCY}__TYPE_OF_DATA={TYPE_OF_DATA}__FOLDS={FOLDS}__CONTEXT_LENGTH/TRAIN_SIZE={CONTEXT_LENGTH}__FT_START_DATE={FT_START_DATE}__START_DATE={START_DATE}__END_DATE={END_DATE}__FT_LENGTH={FT_LENGTH}__DATA_LENGTH={DATA_LENGTH}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"P_L={PREDICTION_LENGTH}__T={TICKER}__FR={FREQUENCY}__T_O_D={TYPE_OF_DATA}__FO={FOLDS}__C_L_T_S={CONTEXT_LENGTH}__FT_S_D={FT_START_DATE}__S_D={START_DATE}__E_D={END_DATE}__FT_L={FT_LENGTH}__D_L={DATA_LENGTH}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_saver.save_results(r, experiment_name, type=\"evaluation\")\n",
    "result_saver.save_results(p, experiment_name, type=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the training data for lag llama fine tuning\n",
    "ft_train_data = lag_llama.prepare_data(data=ft_data, \n",
    "                                       prediction_length=0, \n",
    "                                       frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the lag llama predictor object \n",
    "predictor = lag_llama_ft.get_predictor(prediction_length=PREDICTION_LENGTH, \n",
    "                                       context_length=CONTEXT_LENGTH, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       max_epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning the predictor object\n",
    "predictor = predictor.train(ft_train_data, \n",
    "                            cache_data = True, \n",
    "                            shuffle_buffer_length = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn TSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p, a = get_tscv_results(data = data,\n",
    "                           prediction_horizon=PREDICTION_LENGTH,\n",
    "                           context_length=CONTEXT_LENGTH, \n",
    "                           folds=FOLDS, \n",
    "                           frequency=FREQUENCY, \n",
    "                           predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the summary for each model\n",
    "s = [get_summary(r[i]) for i in range(len(r))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df, medians_df, stds_df = extract_metrics(s, MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.standard_visualisation(MODELS, METRICS, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.interactive_visualisation(MODELS, METRICS, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs.prediction_visualisation(MODELS, p, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL the graph\n",
    "label the axes with timestamps\n",
    "show the history of the actual\n",
    "table of comparison, not just graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
